{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from scipy.misc import imread, imresize\n",
    "import os\n",
    "import matplotlib.pyplot as ply\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "image_data = {}\n",
    "for image_file in os.listdir('data/images'):\n",
    "    image_data[image_file.split(\".\")[0]] = imresize(imread(os.path.join('data/images', image_file)),(128,128)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.head of        id                       species   margin1   margin2   margin3  \\\n",
       "0       1                   Acer_Opalus  0.007812  0.023438  0.023438   \n",
       "1       2         Pterocarya_Stenoptera  0.005859  0.000000  0.031250   \n",
       "2       3          Quercus_Hartwissiana  0.005859  0.009766  0.019531   \n",
       "3       5               Tilia_Tomentosa  0.000000  0.003906  0.023438   \n",
       "4       6            Quercus_Variabilis  0.005859  0.003906  0.048828   \n",
       "5       8          Magnolia_Salicifolia  0.070312  0.093750  0.033203   \n",
       "6      10           Quercus_Canariensis  0.021484  0.031250  0.017578   \n",
       "7      11                 Quercus_Rubra  0.000000  0.000000  0.037109   \n",
       "8      14               Quercus_Brantii  0.005859  0.001953  0.033203   \n",
       "9      15                Salix_Fragilis  0.000000  0.000000  0.009766   \n",
       "10     17               Zelkova_Serrata  0.019531  0.031250  0.001953   \n",
       "11     18         Betula_Austrosinensis  0.001953  0.001953  0.023438   \n",
       "12     20               Quercus_Pontica  0.015625  0.011719  0.041016   \n",
       "13     21                Quercus_Afares  0.011719  0.011719  0.054688   \n",
       "14     22             Quercus_Coccifera  0.011719  0.007812  0.111330   \n",
       "15     25               Fagus_Sylvatica  0.027344  0.025391  0.066406   \n",
       "16     26                   Phildelphus  0.009766  0.062500  0.033203   \n",
       "17     27                 Acer_Palmatum  0.000000  0.000000  0.001953   \n",
       "18     29             Quercus_Pubescens  0.001953  0.000000  0.015625   \n",
       "19     30             Populus_Adenopoda  0.005859  0.027344  0.017578   \n",
       "20     31               Quercus_Trojana  0.019531  0.019531  0.064453   \n",
       "21     32            Quercus_Variabilis  0.019531  0.029297  0.080078   \n",
       "22     34             Alnus_Sieboldiana  0.000000  0.005859  0.023438   \n",
       "23     35                  Quercus_Ilex  0.029297  0.033203  0.033203   \n",
       "24     37           Arundinaria_Simonii  0.001953  0.023438  0.005859   \n",
       "25     38               Acer_Platanoids  0.015625  0.031250  0.101560   \n",
       "26     40        Quercus_Phillyraeoides  0.027344  0.009766  0.021484   \n",
       "27     42              Cornus_Chinensis  0.085938  0.095703  0.007812   \n",
       "28     43        Quercus_Phillyraeoides  0.001953  0.003906  0.017578   \n",
       "29     45               Fagus_Sylvatica  0.019531  0.041016  0.093750   \n",
       "..    ...                           ...       ...       ...       ...   \n",
       "960  1541             Quercus_Vulcanica  0.003906  0.000000  0.013672   \n",
       "961  1543                Salix_Fragilis  0.000000  0.000000  0.007812   \n",
       "962  1544               Tilia_Tomentosa  0.000000  0.017578  0.015625   \n",
       "963  1545         Populus_Grandidentata  0.009766  0.011719  0.121090   \n",
       "964  1547             Prunus_X_Shmittii  0.001953  0.001953  0.007812   \n",
       "965  1548           Quercus_x_Hispanica  0.003906  0.003906  0.033203   \n",
       "966  1549                 Quercus_Suber  0.011719  0.005859  0.048828   \n",
       "967  1550                 Alnus_Viridis  0.000000  0.000000  0.005859   \n",
       "968  1551                 Acer_Palmatum  0.000000  0.000000  0.003906   \n",
       "969  1552           Quercus_Rhysophylla  0.025391  0.042969  0.025391   \n",
       "970  1554                Quercus_Cerris  0.011719  0.013672  0.021484   \n",
       "971  1555                Quercus_Texana  0.000000  0.005859  0.072266   \n",
       "972  1556                     Acer_Mono  0.029297  0.005859  0.056641   \n",
       "973  1557             Quercus_Vulcanica  0.011719  0.005859  0.001953   \n",
       "974  1559                 Quercus_Nigra  0.037109  0.056641  0.019531   \n",
       "975  1561   Rhododendron_x_Russellianum  0.074219  0.093750  0.007812   \n",
       "976  1562               Acer_Capillipes  0.000000  0.000000  0.013672   \n",
       "977  1563             Quercus_Pubescens  0.001953  0.000000  0.041016   \n",
       "978  1566                 Alnus_Viridis  0.000000  0.000000  0.007812   \n",
       "979  1568           Quercus_x_Hispanica  0.009766  0.003906  0.025391   \n",
       "980  1569                 Quercus_Suber  0.031250  0.009766  0.078125   \n",
       "981  1570  Viburnum_x_Rhytidophylloides  0.027344  0.044922  0.023438   \n",
       "982  1571                 Quercus_Nigra  0.017578  0.074219  0.015625   \n",
       "983  1572               Quercus_Phellos  0.023438  0.076172  0.015625   \n",
       "984  1574                  Ilex_Cornuta  0.080078  0.048828  0.023438   \n",
       "985  1575          Magnolia_Salicifolia  0.060547  0.119140  0.007812   \n",
       "986  1578                   Acer_Pictum  0.001953  0.003906  0.021484   \n",
       "987  1581            Alnus_Maximowiczii  0.001953  0.003906  0.000000   \n",
       "988  1582                 Quercus_Rubra  0.000000  0.000000  0.046875   \n",
       "989  1584                Quercus_Afares  0.023438  0.019531  0.031250   \n",
       "\n",
       "      margin4   margin5   margin6   margin7   margin8    ...      texture55  \\\n",
       "0    0.003906  0.011719  0.009766  0.027344  0.000000    ...       0.007812   \n",
       "1    0.015625  0.025391  0.001953  0.019531  0.000000    ...       0.000977   \n",
       "2    0.007812  0.003906  0.005859  0.068359  0.000000    ...       0.154300   \n",
       "3    0.005859  0.021484  0.019531  0.023438  0.000000    ...       0.000000   \n",
       "4    0.009766  0.013672  0.015625  0.005859  0.000000    ...       0.096680   \n",
       "5    0.001953  0.000000  0.152340  0.007812  0.000000    ...       0.145510   \n",
       "6    0.009766  0.001953  0.042969  0.039062  0.000000    ...       0.085938   \n",
       "7    0.050781  0.003906  0.000000  0.003906  0.000000    ...       0.038086   \n",
       "8    0.015625  0.001953  0.000000  0.023438  0.000000    ...       0.000000   \n",
       "9    0.037109  0.072266  0.000000  0.000000  0.000000    ...       0.000000   \n",
       "10   0.005859  0.003906  0.013672  0.033203  0.000000    ...       0.009766   \n",
       "11   0.025391  0.076172  0.000000  0.029297  0.000000    ...       0.013672   \n",
       "12   0.003906  0.023438  0.015625  0.019531  0.000000    ...       0.003906   \n",
       "13   0.017578  0.007812  0.009766  0.011719  0.007812    ...       0.002930   \n",
       "14   0.027344  0.023438  0.039062  0.013672  0.000000    ...       0.000977   \n",
       "15   0.007812  0.003906  0.052734  0.031250  0.000000    ...       0.008789   \n",
       "16   0.029297  0.011719  0.044922  0.005859  0.000000    ...       0.000000   \n",
       "17   0.029297  0.111330  0.000000  0.000000  0.000000    ...       0.012695   \n",
       "18   0.031250  0.011719  0.000000  0.021484  0.001953    ...       0.000000   \n",
       "19   0.041016  0.007812  0.017578  0.046875  0.001953    ...       0.000000   \n",
       "20   0.025391  0.007812  0.005859  0.023438  0.000000    ...       0.000000   \n",
       "21   0.033203  0.025391  0.023438  0.009766  0.005859    ...       0.021484   \n",
       "22   0.001953  0.013672  0.001953  0.015625  0.000000    ...       0.000000   \n",
       "23   0.021484  0.001953  0.068359  0.037109  0.000000    ...       0.000000   \n",
       "24   0.029297  0.000000  0.000000  0.000000  0.000000    ...       0.103520   \n",
       "25   0.015625  0.005859  0.017578  0.000000  0.000000    ...       0.029297   \n",
       "26   0.042969  0.017578  0.009766  0.037109  0.031250    ...       0.000000   \n",
       "27   0.003906  0.000000  0.148440  0.007812  0.000000    ...       0.018555   \n",
       "28   0.044922  0.041016  0.011719  0.039062  0.013672    ...       0.000000   \n",
       "29   0.005859  0.011719  0.037109  0.035156  0.000000    ...       0.076172   \n",
       "..        ...       ...       ...       ...       ...    ...            ...   \n",
       "960  0.029297  0.007812  0.005859  0.019531  0.000000    ...       0.000000   \n",
       "961  0.015625  0.072266  0.000000  0.011719  0.003906    ...       0.000000   \n",
       "962  0.011719  0.037109  0.003906  0.019531  0.000000    ...       0.000000   \n",
       "963  0.003906  0.009766  0.009766  0.007812  0.005859    ...       0.213870   \n",
       "964  0.019531  0.037109  0.000000  0.013672  0.000000    ...       0.000000   \n",
       "965  0.037109  0.003906  0.003906  0.031250  0.000000    ...       0.000000   \n",
       "966  0.009766  0.003906  0.046875  0.050781  0.000000    ...       0.000000   \n",
       "967  0.013672  0.031250  0.000000  0.011719  0.000000    ...       0.000000   \n",
       "968  0.039062  0.082031  0.000000  0.000000  0.000000    ...       0.007812   \n",
       "969  0.033203  0.001953  0.033203  0.011719  0.000000    ...       0.000000   \n",
       "970  0.031250  0.017578  0.003906  0.025391  0.003906    ...       0.000000   \n",
       "971  0.076172  0.007812  0.000000  0.000000  0.000000    ...       0.040039   \n",
       "972  0.031250  0.000000  0.021484  0.031250  0.003906    ...       0.109380   \n",
       "973  0.003906  0.011719  0.011719  0.031250  0.000000    ...       0.066406   \n",
       "974  0.021484  0.001953  0.068359  0.015625  0.000000    ...       0.046875   \n",
       "975  0.013672  0.000000  0.119140  0.003906  0.000000    ...       0.000000   \n",
       "976  0.015625  0.048828  0.000000  0.033203  0.000000    ...       0.094727   \n",
       "977  0.015625  0.015625  0.000000  0.009766  0.000000    ...       0.000000   \n",
       "978  0.007812  0.039062  0.000000  0.001953  0.005859    ...       0.000000   \n",
       "979  0.017578  0.005859  0.003906  0.035156  0.000000    ...       0.000000   \n",
       "980  0.017578  0.003906  0.058594  0.048828  0.000000    ...       0.000977   \n",
       "981  0.017578  0.001953  0.025391  0.083984  0.000000    ...       0.000000   \n",
       "982  0.013672  0.000000  0.074219  0.005859  0.000000    ...       0.025391   \n",
       "983  0.019531  0.000000  0.039062  0.011719  0.000000    ...       0.053711   \n",
       "984  0.007812  0.000000  0.091797  0.009766  0.000000    ...       0.003906   \n",
       "985  0.003906  0.000000  0.148440  0.017578  0.000000    ...       0.242190   \n",
       "986  0.107420  0.001953  0.000000  0.000000  0.000000    ...       0.170900   \n",
       "987  0.021484  0.078125  0.003906  0.007812  0.000000    ...       0.004883   \n",
       "988  0.056641  0.009766  0.000000  0.000000  0.000000    ...       0.083008   \n",
       "989  0.015625  0.005859  0.019531  0.035156  0.000000    ...       0.000000   \n",
       "\n",
       "     texture56  texture57  texture58  texture59  texture60  texture61  \\\n",
       "0     0.000000   0.002930   0.002930   0.035156   0.000000   0.000000   \n",
       "1     0.000000   0.000000   0.000977   0.023438   0.000000   0.000000   \n",
       "2     0.000000   0.005859   0.000977   0.007812   0.000000   0.000000   \n",
       "3     0.000977   0.000000   0.000000   0.020508   0.000000   0.000000   \n",
       "4     0.000000   0.021484   0.000000   0.000000   0.000000   0.000000   \n",
       "5     0.000000   0.041992   0.000000   0.005859   0.000000   0.000000   \n",
       "6     0.000000   0.040039   0.000000   0.009766   0.000000   0.000000   \n",
       "7     0.025391   0.009766   0.002930   0.021484   0.000000   0.037109   \n",
       "8     0.000000   0.008789   0.000000   0.017578   0.000000   0.000000   \n",
       "9     0.000000   0.000000   0.070312   0.013672   0.192380   0.000000   \n",
       "10    0.000000   0.000000   0.002930   0.024414   0.000000   0.000000   \n",
       "11    0.003906   0.014648   0.003906   0.036133   0.000000   0.000000   \n",
       "12    0.000000   0.086914   0.000000   0.013672   0.000000   0.000000   \n",
       "13    0.000000   0.015625   0.000000   0.015625   0.000000   0.000000   \n",
       "14    0.001953   0.000000   0.034180   0.007812   0.038086   0.000000   \n",
       "15    0.000000   0.004883   0.004883   0.009766   0.001953   0.000000   \n",
       "16    0.000000   0.004883   0.000000   0.068359   0.000000   0.000000   \n",
       "17    0.000000   0.079102   0.000000   0.000000   0.000000   0.000000   \n",
       "18    0.000000   0.001953   0.005859   0.049805   0.000000   0.000000   \n",
       "19    0.000977   0.000000   0.020508   0.001953   0.333010   0.000000   \n",
       "20    0.000977   0.001953   0.016602   0.023438   0.004883   0.000000   \n",
       "21    0.000000   0.057617   0.000000   0.002930   0.000000   0.000000   \n",
       "22    0.019531   0.000000   0.031250   0.005859   0.054688   0.000000   \n",
       "23    0.000000   0.000000   0.044922   0.003906   0.004883   0.000000   \n",
       "24    0.009766   0.000000   0.002930   0.002930   0.077148   0.000000   \n",
       "25    0.033203   0.007812   0.006836   0.014648   0.000000   0.000000   \n",
       "26    0.000000   0.003906   0.000977   0.009766   0.001953   0.000000   \n",
       "27    0.052734   0.005859   0.000977   0.019531   0.000000   0.011719   \n",
       "28    0.000000   0.000000   0.000977   0.007812   0.000000   0.000000   \n",
       "29    0.000000   0.049805   0.000000   0.000977   0.000000   0.000000   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "960   0.000000   0.013672   0.000000   0.019531   0.000000   0.000000   \n",
       "961   0.000000   0.000000   0.041016   0.057617   0.011719   0.000000   \n",
       "962   0.002930   0.000000   0.015625   0.029297   0.000000   0.000000   \n",
       "963   0.000000   0.041016   0.000000   0.003906   0.000000   0.000000   \n",
       "964   0.000000   0.003906   0.000977   0.039062   0.000000   0.000000   \n",
       "965   0.000000   0.000000   0.011719   0.028320   0.000000   0.000000   \n",
       "966   0.000000   0.000000   0.039062   0.000977   0.008789   0.000000   \n",
       "967   0.020508   0.002930   0.009766   0.026367   0.000000   0.000000   \n",
       "968   0.000000   0.144530   0.000000   0.000000   0.000000   0.000000   \n",
       "969   0.147460   0.000977   0.018555   0.008789   0.009766   0.000000   \n",
       "970   0.000000   0.002930   0.000000   0.019531   0.000000   0.000000   \n",
       "971   0.000977   0.006836   0.002930   0.022461   0.000000   0.082031   \n",
       "972   0.000000   0.013672   0.004883   0.018555   0.000000   0.000000   \n",
       "973   0.000000   0.096680   0.000000   0.006836   0.000000   0.000000   \n",
       "974   0.000000   0.009766   0.000000   0.010742   0.000000   0.000977   \n",
       "975   0.000000   0.000000   0.005859   0.007812   0.000000   0.000000   \n",
       "976   0.000000   0.016602   0.000000   0.018555   0.000000   0.000000   \n",
       "977   0.000000   0.017578   0.011719   0.070312   0.000000   0.000000   \n",
       "978   0.056641   0.000000   0.035156   0.011719   0.024414   0.000000   \n",
       "979   0.000000   0.000000   0.008789   0.016602   0.000000   0.000000   \n",
       "980   0.000000   0.024414   0.006836   0.017578   0.000977   0.000000   \n",
       "981   0.000000   0.000000   0.036133   0.082031   0.000000   0.000000   \n",
       "982   0.000000   0.003906   0.000000   0.009766   0.000000   0.012695   \n",
       "983   0.000000   0.002930   0.000977   0.005859   0.000000   0.081055   \n",
       "984   0.003906   0.041016   0.006836   0.017578   0.000000   0.000000   \n",
       "985   0.000000   0.034180   0.000000   0.010742   0.000000   0.000000   \n",
       "986   0.000000   0.018555   0.000000   0.011719   0.000000   0.000000   \n",
       "987   0.000977   0.004883   0.027344   0.016602   0.007812   0.000000   \n",
       "988   0.030273   0.000977   0.002930   0.014648   0.000000   0.041992   \n",
       "989   0.000000   0.002930   0.000000   0.012695   0.000000   0.000000   \n",
       "\n",
       "     texture62  texture63  texture64  \n",
       "0     0.004883   0.000000   0.025391  \n",
       "1     0.000977   0.039062   0.022461  \n",
       "2     0.000000   0.020508   0.002930  \n",
       "3     0.017578   0.000000   0.047852  \n",
       "4     0.000000   0.000000   0.031250  \n",
       "5     0.000000   0.001953   0.013672  \n",
       "6     0.000000   0.039062   0.003906  \n",
       "7     0.006836   0.002930   0.036133  \n",
       "8     0.000977   0.033203   0.074219  \n",
       "9     0.074219   0.000000   0.000000  \n",
       "10    0.006836   0.000000   0.004883  \n",
       "11    0.012695   0.005859   0.000000  \n",
       "12    0.000000   0.011719   0.002930  \n",
       "13    0.000000   0.018555   0.022461  \n",
       "14    0.089844   0.023438   0.001953  \n",
       "15    0.003906   0.000000   0.036133  \n",
       "16    0.000000   0.036133   0.000977  \n",
       "17    0.000000   0.000000   0.019531  \n",
       "18    0.006836   0.000000   0.001953  \n",
       "19    0.169920   0.000000   0.000000  \n",
       "20    0.003906   0.013672   0.004883  \n",
       "21    0.000000   0.000000   0.035156  \n",
       "22    0.103520   0.000977   0.000000  \n",
       "23    0.069336   0.000000   0.000000  \n",
       "24    0.052734   0.000000   0.000000  \n",
       "25    0.020508   0.000000   0.035156  \n",
       "26    0.001953   0.023438   0.007812  \n",
       "27    0.041992   0.004883   0.020508  \n",
       "28    0.007812   0.024414   0.015625  \n",
       "29    0.000000   0.000000   0.001953  \n",
       "..         ...        ...        ...  \n",
       "960   0.003906   0.029297   0.055664  \n",
       "961   0.010742   0.000000   0.002930  \n",
       "962   0.030273   0.000000   0.012695  \n",
       "963   0.000000   0.000000   0.014648  \n",
       "964   0.000000   0.022461   0.033203  \n",
       "965   0.011719   0.008789   0.000000  \n",
       "966   0.032227   0.000000   0.000977  \n",
       "967   0.009766   0.011719   0.041016  \n",
       "968   0.000000   0.000000   0.010742  \n",
       "969   0.078125   0.022461   0.000000  \n",
       "970   0.001953   0.010742   0.067383  \n",
       "971   0.001953   0.005859   0.029297  \n",
       "972   0.001953   0.000977   0.004883  \n",
       "973   0.000000   0.030273   0.011719  \n",
       "974   0.005859   0.011719   0.002930  \n",
       "975   0.030273   0.012695   0.000977  \n",
       "976   0.000977   0.028320   0.000000  \n",
       "977   0.000977   0.000000   0.012695  \n",
       "978   0.066406   0.013672   0.000000  \n",
       "979   0.024414   0.037109   0.003906  \n",
       "980   0.016602   0.000000   0.041016  \n",
       "981   0.026367   0.008789   0.000000  \n",
       "982   0.000000   0.009766   0.000000  \n",
       "983   0.000000   0.003906   0.019531  \n",
       "984   0.003906   0.000977   0.032227  \n",
       "985   0.000000   0.000000   0.018555  \n",
       "986   0.000977   0.000000   0.021484  \n",
       "987   0.027344   0.000000   0.001953  \n",
       "988   0.000000   0.001953   0.002930  \n",
       "989   0.023438   0.025391   0.022461  \n",
       "\n",
       "[990 rows x 194 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_images():\n",
    "    dr = \"data/images/\"\n",
    "    ims = [Image.open(dr + f) for f in os.listdir(dr) if f.endswith('jpg')]\n",
    "    max_height = max(i.height for i in ims)\n",
    "    max_width = max(i.width for i in ims)\n",
    "    newims = []\n",
    "    img_rows, img_cols = 0, 0\n",
    "    for im in ims[:1]:\n",
    "        new_im = im.crop((0, 0, max_width, max_height))\n",
    "        print new_im.size\n",
    "\n",
    "        new_im.thumbnail((100,100))\n",
    "        print new_im.size\n",
    "\n",
    "\n",
    "        img_rows, img_cols = max(img_rows, new_im.height), max(img_cols, new_im.width)\n",
    "        \n",
    "        newims.append(np.asarray(new_im))\n",
    "    print(img_rows,img_cols)\n",
    "    return np.stack(newims),img_rows, img_cols\n",
    "ims, img_row, img_col = load_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_data(train, image_data, test):\n",
    "    le = LabelEncoder().fit(train.species)\n",
    "    labels = le.transform(train.species)\n",
    "    labels_cat = to_categorical(labels)\n",
    "    classes = list(le.classes_)\n",
    "\n",
    "    test_ids = test.id\n",
    "    train_ids = train.id\n",
    "    image_train = np.array([image_data[str(_)] for _ in train_ids])\n",
    "    image_train = image_train.reshape((image_train.shape[0],1,256,256))\n",
    "    image_test = np.array([image_data[str(_)] for _ in test_ids])\n",
    "    image_test = image_test.reshape((image_test.shape[0],1,256,256))\n",
    "    \n",
    "    train = train.drop(['id', 'species'], axis=1)\n",
    "    test = test.drop(['id'], axis=1)\n",
    "\n",
    "    scaler = StandardScaler().fit(train)\n",
    "    train = scaler.transform(train)\n",
    "    test = scaler.transform(test)\n",
    "\n",
    "\n",
    "    return train, labels_cat, classes, test_ids, test, image_train, image_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'species'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c595abee6588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-4b61da56d6b7>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(train, image_data, test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabels_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'species'"
     ]
    }
   ],
   "source": [
    "train, labels, classes, test_ids, test, image_train, image_test = prepare_data(train, image_data, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sss = cross_validation.StratifiedShuffleSplit(labels, 1, test_size=0.2, random_state=23 )\n",
    "\n",
    "for tr, ts in sss:\n",
    "    train_x, test_x = train[tr], train[ts]\n",
    "    train_y, test_y = labels[tr], labels[ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Score: 0.77232. Rank: 326\n",
    "rf = RandomForestClassifier(n_estimators=70, max_features=14, min_samples_split=1)\n",
    "rf = rf.fit(train_x, train_y)\n",
    "rf.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_score = cross_validation.cross_val_score(rf, train, labels, cv=5)\n",
    "print cv_score\n",
    "print np.mean(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = rf.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_submit(preds):\n",
    "    submission = pd.DataFrame(preds, columns=classes)\n",
    "    submission.insert(0, 'id', test_ids)\n",
    "    submission.reset_index()\n",
    "    submission.to_csv('data/submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ply.imshow(image_data['250'],cmap='Set3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Merge\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "\n",
    "#experiment with dropouts\n",
    "img_model = Sequential()\n",
    "img_model.add(Convolution2D(64, 5, 5, border_mode=\"same\", input_shape=(1, 256, 256)))\n",
    "img_model.add(Activation(\"relu\"))\n",
    "img_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), border_mode=\"same\"))\n",
    "img_model.add(Convolution2D(32, 5, 5, border_mode=\"same\"))\n",
    "img_model.add(Activation(\"relu\"))\n",
    "img_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), border_mode=\"same\"))\n",
    "img_model.add(Flatten())\n",
    "img_model.add(Dense(1024))\n",
    "img_model.add(Activation(\"relu\"))\n",
    "img_model.add(Dropout(0.5))\n",
    "\n",
    "feat_model = Sequential()\n",
    "feat_model.add(Dense(1024, input_dim=192))\n",
    "feat_model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model = Sequential()\n",
    "#try dot product\n",
    "model.add(Merge([img_model, feat_model], mode='concat'))              \n",
    "model.add(Dense(512))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(99))\n",
    "model.add(Activation(\"softmax\"))\n",
    "if os.path.exist(\"weights.h5\"):\n",
    "    model.load_weights(\"weights.h5\")\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit([image_train, train], labels, nb_epoch=100, batch_size=128)\n",
    "model.save_weights(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = model.predict([image_test, test])\n",
    "make_submit(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
